---
layout: article
title:  "Mixture Density Networks"
date:   2018-09-02 18:00:00
categories: blog ai

image:
    teaser: mdn/mdn_teaser.gif
    feature: mdn/mdn_feature.png
---

{% include toc.html %}

Some kind of patterns can not be captured with an ordinary feedfoward neuronal network. Take for example the function plotted below. Well it is not a function in mathematical sense, because the mapping $X \mapsto Y$ is ambiguous. So we can not use a function $X \mapsto Y$ to regress such a pattern.

![MDN prediction](/images/mdn/mdn_prediction.png)

Mixture Density Networks (MDN) model this pattern with a mixture of Gaussian distributions. The loss fucntion for this network can be derived from minimizing the negative log-likelihood of the data, w.r.t. the mixture of Gaussian distributions.

\begin{equation}
\mathcal{L}(y,x) = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{equation}

We can benefit from implementations already provided in `tf.contrib.distributions` for both, the **categorical distribution** $\Pi_k(x)$

```python
tf.contrib.distributions.OneHotCategorical(probs= [0.1, 0.5, 0.4])
```

and the Gaussian parts $\mathcal{N}$ of the mixture

```python
tf.contrib.distributions.Normal(loc= 0.0, scale= 1.0)
```

---

We use a neuronal network to regress the parameters of the distributions:
We want $\pi$ to be a categorical distribution summing to one. This can be achieved by soft-max loss. The soft max loss for the logits $z$ of the mixture distribution $\pi$ is:
\begin{equation}
\pi_j = \sigma(z_j) = \frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}}
\end{equation}

For numerical stability reasons we do not regress $\pi$ directly with the network, but the probabilities in log-space $\log(\pi)$. This simplifies the soft-max as follows:

\begin{align}
\forall_j: \; \log(\pi_j) = \log\big(\sigma(z_j)\big) & = \log\big(\frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}} \big)
\end{align}

\begin{align}
= \log\big(e^{z_j} \big) - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}

\begin{align}
= z_j - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}

The second part of the formula can be computed numerically stable with

```python
	tf.reduce_logsumexp(...)
```

Let's see how the loss changes with these adaptions:

\begin{align}
\mathcal{L}(y,x) & = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}

\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big)} \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}

\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big) + \log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) } \Big]
\end{align}

The probability of the Gaussian in log-space simplifies to:
\begin{align}
\log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) &= \log(\frac{1}{\sigma \sqrt{2\pi}}) + \log\Big(e^{- 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2} \Big)
\end{align}

\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2
\end{align}

\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{\sigma ^2}
\end{align}

\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}

\begin{align}
= -log(\sigma) - \log(\sqrt{2\pi}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}

The loss is now a function of $\log(\sigma), \log(\Pi)$ and $ \mu$ which we regress with the neural network.

---
Additionally we can add a temperature parameter $\tau$ to control the uncertainty of our learned model as you can see below.
![MDN temperature](/images/mdn/mdn_temperatures.gif)

---

If we cut a slice of the plot for $X=0$ we can see the learned mixture of Gaussians for the distribution $\mathcal{P}(Y)$ over outcomes $Y$ :
![pdf](/images/mdn/mixture_pdf.png)

