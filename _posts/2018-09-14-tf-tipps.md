---
layout: article
title:  "Guide for better Tensorflow"
date:   2018-09-14 18:00:00
categories: blog ai

image:
    teaser: tf_teaser.gif
    feature: tf.png
---

{% include toc.html %}

Although Tensorflow is well documented for its basic features, the documentation becomes rather sparse when coming to more advanced topics.
This blog post is a collection of best practice gathered from various Stackoverflow posts and Github issues over time.
Due to the frequently changing Tensorflow API, I can not guarantee that the ideas presented in this post will stay best pratice for a lon time.

Furthermore the aim of this post is not to introduce the basics of Tensorflow, as there already a great documentation exists.
It's for people that are already familiar with Tensorflow and want to get inspired on how to write more efficient code.

# 1 Graph Definition vs. Execution
Well, to be honest: this is not really an advanced topic. 
But to write efficient Tensorflow code it is important to distinguish between graph definition and its execution: 
We use **Python** to describe a computational graph. 
This graph is actually executed in a session with fast **C++** implementations.
So whenever calling a function from `tf.` module you add a operation-node to the definition of the graph.

I like to do all the graph definitions in python classes:

```python
class Model():

    def __init__(self, input):
        self.input = input

        # Calls the graph definition once
        self.__define_network()
    
    def __define_network(self):

        # Use class properties to keep track of the references to Tensorflow Ops
        self.logits = ...
        self.loss = ...
```

This allows clean organisation, because all tensors belonging to a particular model are properties of a class instance e.g. `model.loss`.
Furthermore it is easy to build larger models out of subcomponents and keeping track of all the losses for subcomponents.

Once you have done all your graph definition it is important to [finalize](https://www.tensorflow.org/api_docs/python/tf/Graph#finalize) your graph with `tf.get_default_graph().finalize()`. When using a [MonitoredTrainingSession](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession) this is already done for you.

**Conclusion:**  
Keep a clean separation between graph definition and its execution in your code. Adding nodes at runtime is slow. Use `tf.get_default_graph().finalize()`.
{: .notice-success}

# 2 Tensor Shapes
With the difference of graph definition and execution in mind I want to continue with tensor shapes. Especially when creating flexible multilayer models it is important to know the shape of tensors. In Tensorflow there are two options to do this:

1. `tf.shape(tensor)`
2. `tensor.get_shape()`

The difference between this two is that `get_shape()` is a property of the python object which encapsulates the operation node in the graph. So we just query the shape that is stored with the node while defining the graph. On the other hand with `tf.shape()` we create a new operation node which computes the shape of another node when evaluated. But this evaluation is performed when running a session. This is also sometimes referred as **static** vs. **dynamic** shape of a tensor.

As long as a static shape is defined (it can be `None` if it is not clear before runtime) it is good pratices to make use of it, e.g.:

```python
num_features = inputs.get_shape().as_list()[-1]
```

**Conclusion:**  
Make use of static shape.
{: .notice-success}

# 3 Monitored Training Session
<iframe src="https://giphy.com/embed/ZKw0NDn10ljSE" width="240" height="174" frameBorder="0" class="giphy-embed" allowFullScreen align="right"></iframe>
To setup a good training session with logging and saving checkpoints can be a lot of work. The good news is that Tensorflow provides a implementation doing most of these things for us: the [MonitoredTrainingSession](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession).
It also automatically recovers from a checkpoint if there already exists one or initializes all the variables otherwise.
The usage is really simple:

```python
with tf.train.MonitoredTrainingSession() as sess:
    sess.run(...)
```

In the following I will asume the use of such a session for running the graph computations.

**Conclusion:**   
`tf.train.MonitoredTrainingSession()` makes life much easier.
{: .notice-success}

# 4 Hooks
A monitored session like above is trivial to use as long as you only need standard behaviour. Tensorflow provides a concept called [Hooks](https://www.tensorflow.org/api_guides/python/train#Training_Hooks) to customize behaviour in various stages of the training procedure.
Like a hook you can link a peace of code to a particular stage of the session:

- `begin()`
- `after_create_session()`
- `before_run()`
- `after_run()`
- `end()`

In which order hooks are called by a monitored session can be found in detail [here](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession).

# 5 Efficient Input Pipeline

Don't use tf.placeholder()
{: .notice-warning}

# 6 Switching Datasets

# 7 Scaffold

# 8 Embedding Projector

