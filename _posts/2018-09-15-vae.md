---
layout: article
title:  "Variational Autoencoders"
date:   2018-09-15 10:00:00
categories: blog ai

image:
    teaser: vae/vae_teaser.gif
    feature: vae/vae_feature.png
---

{% include toc.html %}

In Machine Learning the Manifold hypothesis says that meaningful data is often located in a low-dimensional manifold of high-dimensional input data.

Learned 2D latent space with ordinary autoencoder:
![Learned Latent Space of AE](/images/vae/mnist_ae.png)

Learned 2D latent space with variational autoencoder:
![Learned Latent Space of VAE](/images/vae/mnist_vae.png)

Examples for Reconstructions of the Autoencoder:
![MNIST digit](/images/vae/mnist_7.png)
![MNIST reconstruction](/images/vae/recon_7.png)

---
Next we want to use a VAE to encode a manifold for human faces. Furthermore we want to apply meaningful transitions in latent space to add features to images.
![Face example](/images/vae/face.png)
![Face reconstruction](/images/vae/face_recon.png)

