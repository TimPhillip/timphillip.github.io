<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="/atom.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-09-02T16:14:58+02:00</updated><id>/</id><title>Tim Schneider</title><subtitle>personal blog</subtitle><entry><title>Predictive Maintenance using LSTM Autoencoder</title><link href="/blog/ai/predictive-maintenance/" rel="alternate" type="text/html" title="Predictive Maintenance using LSTM Autoencoder" /><published>2018-08-31T00:00:00+02:00</published><updated>2018-08-31T00:00:00+02:00</updated><id>/blog/ai/predictive-maintenance</id><content type="html" xml:base="/blog/ai/predictive-maintenance/">&lt;nav class=&quot;toc&quot;&gt;

&lt;/nav&gt;

&lt;h5 id=&quot;predictive-maintenance&quot;&gt;Predictive Maintenance&lt;/h5&gt;
&lt;p&gt;Nowadays machines are most likely maintained in fixed service intervals. This might lead to high inspection costs and even higher costs, when the machine breaks before the next routine service check. A big promise of Industry 4.0 and collecting sensor readings in terms of Big Data is to avoid such unecessary maintainance by predicting a service time interval that fits right the condition of your machine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Recently I read a &lt;a href=&quot;https://arxiv.org/abs/1608.06154&quot;&gt;paper by Malhotra et. al.&lt;/a&gt; about how to use &lt;strong&gt;LSTM Autoencoders&lt;/strong&gt; to predict the &lt;strong&gt;R&lt;/strong&gt;emaining &lt;strong&gt;U&lt;/strong&gt;seful &lt;strong&gt;L&lt;/strong&gt;ife (RUL) of a machine. The basic idea is to use an &lt;strong&gt;Autoencoder&lt;/strong&gt;, which is trained on healthy state, as a measure of anomaly for an arbitray sequence of sensor readings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;turbofan-sensor-data-from-nasa&quot;&gt;Turbofan Sensor Data from Nasa&lt;/h4&gt;

&lt;p&gt;&lt;img align=&quot;left&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/44/Turbofan3_Unlabelled.gif&quot; width=&quot;25%&quot; style=&quot;margin:10px&quot; /&gt;
A &lt;a href=&quot;https://en.wikipedia.org/wiki/Turbofan&quot;&gt;turbofan&lt;/a&gt; is a kind a of jet engine that is widely used in aviation. It’s critical to avoid failure and keep maintenance costs low at the same time. So predicting the remaining uselful life of an engine would be useful.&lt;/p&gt;

&lt;p&gt;&lt;img align=&quot;right&quot; src=&quot;https://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg&quot; /&gt;
The algorithm is trained on the &lt;a href=&quot;https://c3.nasa.gov/dashlink/resources/139/&quot;&gt;NASA Turbofan Degradation Dataset&lt;/a&gt;.
In this dataset engine degradation is carried out until failure while recording many sensor readings.
The Task is to predict the RUL of an unseen engine’s sensor readings.&lt;/p&gt;

&lt;h4 id=&quot;derived-sensor-readings&quot;&gt;Derived Sensor Readings&lt;/h4&gt;
&lt;p&gt;The autoencoder in the paper is not directly trained on sensor readings. For training they use so called &lt;strong&gt;derived sensors&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt;, which are basically the (first 3) principle components of the entire sensor reading data.
These sensors are determined by performing a &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;Principle Component Analysis (PCA)&lt;/a&gt; on the centered sensor readings. With this the dimensionalty of the data is reduced and (linear) correlation between sensors are removed.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/example_pca.png&quot; width=&quot;40%&quot; /&gt;
    &lt;figcaption&gt;Example for Principle Components derived by PCA&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;lstm-autoencoder&quot;&gt;LSTM Autoencoder&lt;/h4&gt;
&lt;p&gt;Train an LSTM Autoencoder to predict these derived sensor readings &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt; of a machine in healthy state. It is assumed that the instance is in healthy condition during the first steps, so using cropped sequences from the start of the training data does the job.
Once trained, let the autoencoder reconstruct the entire series and record the reconstruction error $e^{(u)}_t$.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/autoencoder.png&quot; width=&quot;70%&quot; /&gt;
    &lt;figcaption&gt;Processing an example sequence of (derived) sensor readings $\langle z_1, z_2, z_3 \rangle$ with the autoencoder. Illustration is taken from the original paper.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;\begin{equation}
e^{(u)}_t = || z^{(u)}_t - z’^{(u)}_t ||_2^2
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;health-index-hi&quot;&gt;Health Index (HI)&lt;/h4&gt;
&lt;p&gt;Based on the reconstruction error $e^{(u)}_t$ one can define a &lt;strong&gt;health index (HI)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;\begin{equation}
h^{(u)}_t = \frac{e^{(u)}_M - e^{(u)}_t}{e^{(u)}_M - e^{(u)}_m}
\end{equation}
with $e^{(u)}_M$ denoting the maximum error and $e^{(u)}_m$ denoting the minimum error.&lt;/p&gt;

&lt;p&gt;See that the reconstruction error is correlated with the machine’s health or lifetime respectively. The plots can be resproduced with the code provided in the repository.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/normalized_hi.pdf&quot; width=&quot;80%&quot; /&gt;
    &lt;figcaption&gt;Reconstruction error based health index averaged over all training instances (and normalized by their lifetime). The errorbars indicate one standard deviation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The actual RUL is estimated by function matching with all the HI curves in the training set. (This part is not in my implementation for now)&lt;/p&gt;

&lt;p&gt;Checkout my &lt;strong&gt;Tensorflow&lt;/strong&gt; implementation on &lt;a href=&quot;https://github.com/TimPhillip/rul_estimate&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Just run
&lt;code class=&quot;highlighter-rouge&quot;&gt;shell
python -m rul_estimate.turbofan_rul --train_ae
&lt;/code&gt;
to train the autoencoder part.&lt;/p&gt;

&lt;p&gt;Afterwards estimate the Health Index for the training instances with
&lt;code class=&quot;highlighter-rouge&quot;&gt;shell
python -m rul_estimate.turbofan_rul --target_hi
&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;notice-info&quot;&gt;&lt;strong&gt;Notice:&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;.notice-info&lt;/code&gt; All ideas presented in this post are taken from the &lt;a href=&quot;https://arxiv.org/abs/1608.06154&quot;&gt;paper by Malhotra et. al.&lt;/a&gt;. All implementations are reproductions based on my understanding of the paper.&lt;/p&gt;</content><summary></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;teaser&quot;=&gt;&quot;turbofan.gif&quot;, &quot;feature&quot;=&gt;&quot;turbofan.png&quot;, &quot;credit&quot;=&gt;&quot;Photo by Inspirationfeed on Unsplash&quot;}" /></entry></feed>
