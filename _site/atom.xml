<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="/atom.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-09-22T09:38:49+02:00</updated><id>/</id><title>Tim Schneider</title><subtitle>personal blog</subtitle><entry><title>Variational Autoencoders</title><link href="/blog/ai/vae/" rel="alternate" type="text/html" title="Variational Autoencoders" /><published>2018-09-15T12:00:00+02:00</published><updated>2018-09-15T12:00:00+02:00</updated><id>/blog/ai/vae</id><content type="html" xml:base="/blog/ai/vae/">&lt;nav class=&quot;toc&quot;&gt;

&lt;/nav&gt;

&lt;p&gt;In Machine Learning the Manifold hypothesis says that meaningful data is often located in a low-dimensional manifold of high-dimensional input data.&lt;/p&gt;

&lt;p&gt;Learned 2D latent space with ordinary autoencoder:
&lt;img src=&quot;/images/vae/mnist_ae.png&quot; alt=&quot;Learned Latent Space of AE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Learned 2D latent space with variational autoencoder:
&lt;img src=&quot;/images/vae/mnist_vae.png&quot; alt=&quot;Learned Latent Space of VAE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Examples for Reconstructions of the Autoencoder:
&lt;img src=&quot;/images/vae/mnist_7.png&quot; alt=&quot;MNIST digit&quot; /&gt;
&lt;img src=&quot;/images/vae/recon_7.png&quot; alt=&quot;MNIST reconstruction&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Next we want to use a VAE to encode a manifold for human faces. Furthermore we want to apply meaningful transitions in latent space to add features to images.
&lt;img src=&quot;/images/vae/face.png&quot; alt=&quot;Face example&quot; /&gt;
&lt;img src=&quot;/images/vae/face_recon.png&quot; alt=&quot;Face reconstruction&quot; /&gt;&lt;/p&gt;</content><summary></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;teaser&quot;=&gt;&quot;vae/vae_teaser.gif&quot;, &quot;feature&quot;=&gt;&quot;vae/vae_feature.png&quot;}" /></entry><entry><title>Guide for better Tensorflow</title><link href="/blog/ai/tf-tipps/" rel="alternate" type="text/html" title="Guide for better Tensorflow" /><published>2018-09-14T20:00:00+02:00</published><updated>2018-09-14T20:00:00+02:00</updated><id>/blog/ai/tf-tipps</id><content type="html" xml:base="/blog/ai/tf-tipps/">&lt;nav class=&quot;toc&quot;&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#graph-definition-vs-execution&quot; id=&quot;markdown-toc-graph-definition-vs-execution&quot;&gt;1 Graph Definition vs. Execution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tensor-shapes&quot; id=&quot;markdown-toc-tensor-shapes&quot;&gt;2 Tensor Shapes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#monitored-training-session&quot; id=&quot;markdown-toc-monitored-training-session&quot;&gt;3 Monitored Training Session&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hooks&quot; id=&quot;markdown-toc-hooks&quot;&gt;4 Hooks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#efficient-input-pipeline&quot; id=&quot;markdown-toc-efficient-input-pipeline&quot;&gt;5 Efficient Input Pipeline&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#switching-datasets&quot; id=&quot;markdown-toc-switching-datasets&quot;&gt;6 Switching Datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scaffold&quot; id=&quot;markdown-toc-scaffold&quot;&gt;7 Scaffold&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#embedding-projector&quot; id=&quot;markdown-toc-embedding-projector&quot;&gt;8 Embedding Projector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/nav&gt;

&lt;p&gt;Although Tensorflow is well documented for its basic features, the documentation becomes rather sparse when coming to more advanced topics.
This blog post is a collection of best practice gathered from various Stackoverflow posts and Github issues over time.
Due to the frequently changing Tensorflow API, I can not guarantee that the ideas presented in this post will stay best pratice for a lon time.&lt;/p&gt;

&lt;p&gt;Furthermore the aim of this post is not to introduce the basics of Tensorflow, as there already a great documentation exists.
It’s for people that are already familiar with Tensorflow and want to get inspired on how to write more efficient code.&lt;/p&gt;

&lt;h1 id=&quot;graph-definition-vs-execution&quot;&gt;1 Graph Definition vs. Execution&lt;/h1&gt;
&lt;p&gt;Well, to be honest: this is not really an advanced topic. 
But to write efficient Tensorflow code it is important to distinguish between graph definition and its execution: 
We use &lt;strong&gt;Python&lt;/strong&gt; to describe a computational graph. 
This graph is actually executed in a session with fast &lt;strong&gt;C++&lt;/strong&gt; implementations.
So whenever calling a function from &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.&lt;/code&gt; module you add a operation-node to the definition of the graph.&lt;/p&gt;

&lt;p&gt;I like to do all the graph definitions in python classes:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Calls the graph definition once&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__define_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__define_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# Use class properties to keep track of the references to Tensorflow Ops&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This allows clean organisation, because all tensors belonging to a particular model are properties of a class instance e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;model.loss&lt;/code&gt;.
Furthermore it is easy to build larger models out of subcomponents and keeping track of all the losses for subcomponents.&lt;/p&gt;

&lt;p&gt;Once you have done all your graph definition it is important to &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#finalize&quot;&gt;finalize&lt;/a&gt; your graph with &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.get_default_graph().finalize()&lt;/code&gt;. When using a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession&quot;&gt;MonitoredTrainingSession&lt;/a&gt; this is already done for you.&lt;/p&gt;

&lt;p class=&quot;notice-success&quot;&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;br /&gt;
Keep a clean separation between graph definition and its execution in your code. Adding nodes at runtime is slow. Use &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.get_default_graph().finalize()&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;tensor-shapes&quot;&gt;2 Tensor Shapes&lt;/h1&gt;
&lt;p&gt;With the difference of graph definition and execution in mind I want to continue with tensor shapes. Especially when creating flexible multilayer models it is important to know the shape of tensors. In Tensorflow there are two options to do this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.shape(tensor)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tensor.get_shape()&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The difference between this two is that &lt;code class=&quot;highlighter-rouge&quot;&gt;get_shape()&lt;/code&gt; is a property of the python object which encapsulates the operation node in the graph. So we just query the shape that is stored with the node while defining the graph. On the other hand with &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.shape()&lt;/code&gt; we create a new operation node which computes the shape of another node when evaluated. But this evaluation is performed when running a session. This is also sometimes referred as &lt;strong&gt;static&lt;/strong&gt; vs. &lt;strong&gt;dynamic&lt;/strong&gt; shape of a tensor.&lt;/p&gt;

&lt;p&gt;As long as a static shape is defined (it can be &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; if it is not clear before runtime) it is good pratices to make use of it, e.g.:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p class=&quot;notice-success&quot;&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;br /&gt;
Make use of static shape.&lt;/p&gt;

&lt;h1 id=&quot;monitored-training-session&quot;&gt;3 Monitored Training Session&lt;/h1&gt;
&lt;iframe src=&quot;https://giphy.com/embed/ZKw0NDn10ljSE&quot; width=&quot;240&quot; height=&quot;174&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot; align=&quot;right&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;To setup a good training session with logging and saving checkpoints can be a lot of work. The good news is that Tensorflow provides a implementation doing most of these things for us: the &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession&quot;&gt;MonitoredTrainingSession&lt;/a&gt;.
It also automatically recovers from a checkpoint if there already exists one or initializes all the variables otherwise.
The usage is really simple:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MonitoredTrainingSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In the following I will asume the use of such a session for running the graph computations.&lt;/p&gt;

&lt;p class=&quot;notice-success&quot;&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.train.MonitoredTrainingSession()&lt;/code&gt; makes life much easier.&lt;/p&gt;

&lt;h1 id=&quot;hooks&quot;&gt;4 Hooks&lt;/h1&gt;
&lt;p&gt;A monitored session like above is trivial to use as long as you only need standard behaviour. Tensorflow provides a concept called &lt;a href=&quot;https://www.tensorflow.org/api_guides/python/train#Training_Hooks&quot;&gt;Hooks&lt;/a&gt; to customize behaviour in various stages of the training procedure.
Like a hook you can link a peace of code to a particular stage of the session:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;begin()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;after_create_session()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;before_run()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;after_run()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;end()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In which order hooks are called by a monitored session can be found in detail &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;efficient-input-pipeline&quot;&gt;5 Efficient Input Pipeline&lt;/h1&gt;

&lt;p class=&quot;notice-warning&quot;&gt;Don’t use tf.placeholder()&lt;/p&gt;

&lt;h1 id=&quot;switching-datasets&quot;&gt;6 Switching Datasets&lt;/h1&gt;

&lt;h1 id=&quot;scaffold&quot;&gt;7 Scaffold&lt;/h1&gt;

&lt;h1 id=&quot;embedding-projector&quot;&gt;8 Embedding Projector&lt;/h1&gt;</content><summary></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;teaser&quot;=&gt;&quot;tf_teaser.gif&quot;, &quot;feature&quot;=&gt;&quot;tf.png&quot;}" /></entry><entry><title>Mixture Density Networks</title><link href="/blog/ai/mdn/" rel="alternate" type="text/html" title="Mixture Density Networks" /><published>2018-09-02T20:00:00+02:00</published><updated>2018-09-02T20:00:00+02:00</updated><id>/blog/ai/mdn</id><content type="html" xml:base="/blog/ai/mdn/">&lt;nav class=&quot;toc&quot;&gt;

&lt;/nav&gt;

&lt;p&gt;Some kind of patterns can not be captured with an ordinary feedfoward neuronal network. Take for example the function plotted below. Well it is not a function in mathematical sense, because the mapping $X \mapsto Y$ is ambiguous. So we can not use a function $X \mapsto Y$ to regress such a pattern.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mdn/mdn_prediction.png&quot; alt=&quot;MDN prediction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mixture Density Networks (MDN) model this pattern with a mixture of Gaussian distributions. The loss fucntion for this network can be derived from minimizing the negative log-likelihood of the data, w.r.t. the mixture of Gaussian distributions.&lt;/p&gt;

&lt;p&gt;\begin{equation}
\mathcal{L}(y,x) = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{equation}&lt;/p&gt;

&lt;p&gt;We can benefit from implementations already provided in &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.contrib.distributions&lt;/code&gt; for both, the &lt;strong&gt;categorical distribution&lt;/strong&gt; $\Pi_k(x)$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OneHotCategorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and the Gaussian parts $\mathcal{N}$ of the mixture&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contrib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distributions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;We use a neuronal network to regress the parameters of the distributions:
We want $\pi$ to be a categorical distribution summing to one. This can be achieved by soft-max loss. The soft max loss for the logits $z$ of the mixture distribution $\pi$ is:
\begin{equation}
\pi_j = \sigma(z_j) = \frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}}
\end{equation}&lt;/p&gt;

&lt;p&gt;For numerical stability reasons we do not regress $\pi$ directly with the network, but the probabilities in log-space $\log(\pi)$. This simplifies the soft-max as follows:&lt;/p&gt;

&lt;p&gt;\begin{align}
\forall_j: \; \log(\pi_j) = \log\big(\sigma(z_j)\big) &amp;amp; = \log\big(\frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}} \big)
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= \log\big(e^{z_j} \big) - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= z_j - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}&lt;/p&gt;

&lt;p&gt;The second part of the formula can be computed numerically stable with&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_logsumexp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Let’s see how the loss changes with these adaptions:&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{L}(y,x) &amp;amp; = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big)} \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big) + \log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) } \Big]
\end{align}&lt;/p&gt;

&lt;p&gt;The probability of the Gaussian in log-space simplifies to:
\begin{align}
\log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) &amp;amp;= \log(\frac{1}{\sigma \sqrt{2\pi}}) + \log\Big(e^{- 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2} \Big)
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{\sigma ^2}
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}&lt;/p&gt;

&lt;p&gt;\begin{align}
= -log(\sigma) - \log(\sqrt{2\pi}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}&lt;/p&gt;

&lt;p&gt;The loss is now a function of $\log(\sigma), \log(\Pi)$ and $ \mu$ which we regress with the neural network.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Additionally we can add a temperature parameter $\tau$ to control the uncertainty of our learned model as you can see below.
&lt;img src=&quot;/images/mdn/mdn_temperatures.gif&quot; alt=&quot;MDN temperature&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If we cut a slice of the plot for $X=0$ we can see the learned mixture of Gaussians for the distribution $\mathcal{P}(Y)$ over outcomes $Y$ :
&lt;img src=&quot;/images/mdn/mixture_pdf.png&quot; alt=&quot;pdf&quot; /&gt;&lt;/p&gt;</content><summary></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;teaser&quot;=&gt;&quot;mdn/mdn_teaser.gif&quot;, &quot;feature&quot;=&gt;&quot;mdn/mdn_feature.png&quot;}" /></entry><entry><title>Predictive Maintenance using LSTM Autoencoder</title><link href="/blog/ai/predictive-maintenance/" rel="alternate" type="text/html" title="Predictive Maintenance using LSTM Autoencoder" /><published>2018-08-31T00:00:00+02:00</published><updated>2018-08-31T00:00:00+02:00</updated><id>/blog/ai/predictive-maintenance</id><content type="html" xml:base="/blog/ai/predictive-maintenance/">&lt;nav class=&quot;toc&quot;&gt;

&lt;/nav&gt;

&lt;h5 id=&quot;predictive-maintenance&quot;&gt;Predictive Maintenance&lt;/h5&gt;
&lt;p&gt;Nowadays machines are most likely maintained in fixed service intervals. This might lead to high inspection costs and even higher costs, when the machine breaks before the next routine service check. A big promise of Industry 4.0 and collecting sensor readings in terms of Big Data is to avoid such unecessary maintainance by predicting a service time interval that fits right the condition of your machine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Recently I read a &lt;a href=&quot;https://arxiv.org/abs/1608.06154&quot;&gt;paper by Malhotra et. al.&lt;/a&gt; about how to use &lt;strong&gt;LSTM Autoencoders&lt;/strong&gt; to predict the &lt;strong&gt;R&lt;/strong&gt;emaining &lt;strong&gt;U&lt;/strong&gt;seful &lt;strong&gt;L&lt;/strong&gt;ife (RUL) of a machine. The basic idea is to use an &lt;strong&gt;Autoencoder&lt;/strong&gt;, which is trained on healthy state, as a measure of anomaly for an arbitray sequence of sensor readings.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;turbofan-sensor-data-from-nasa&quot;&gt;Turbofan Sensor Data from Nasa&lt;/h4&gt;

&lt;p&gt;&lt;img align=&quot;left&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/4/44/Turbofan3_Unlabelled.gif&quot; width=&quot;25%&quot; style=&quot;margin:10px&quot; /&gt;
A &lt;a href=&quot;https://en.wikipedia.org/wiki/Turbofan&quot;&gt;turbofan&lt;/a&gt; is a kind a of jet engine that is widely used in aviation. It’s critical to avoid failure and keep maintenance costs low at the same time. So predicting the remaining uselful life of an engine would be useful.&lt;/p&gt;

&lt;p&gt;&lt;img align=&quot;right&quot; src=&quot;https://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg&quot; /&gt;
The algorithm is trained on the &lt;a href=&quot;https://c3.nasa.gov/dashlink/resources/139/&quot;&gt;NASA Turbofan Degradation Dataset&lt;/a&gt;.
In this dataset engine degradation is carried out until failure while recording many sensor readings.
The Task is to predict the RUL of an unseen engine’s sensor readings.&lt;/p&gt;

&lt;h4 id=&quot;derived-sensor-readings&quot;&gt;Derived Sensor Readings&lt;/h4&gt;
&lt;p&gt;The autoencoder in the paper is not directly trained on sensor readings. For training they use so called &lt;strong&gt;derived sensors&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt;, which are basically the (first 3) principle components of the entire sensor reading data.
These sensors are determined by performing a &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;Principle Component Analysis (PCA)&lt;/a&gt; on the centered sensor readings. With this the dimensionalty of the data is reduced and (linear) correlation between sensors are removed.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/example_pca.png&quot; width=&quot;40%&quot; /&gt;
    &lt;figcaption&gt;Example for Principle Components derived by PCA&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;lstm-autoencoder&quot;&gt;LSTM Autoencoder&lt;/h4&gt;
&lt;p&gt;Train an LSTM Autoencoder to predict these derived sensor readings &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt; of a machine in healthy state. It is assumed that the instance is in healthy condition during the first steps, so using cropped sequences from the start of the training data does the job.
Once trained, let the autoencoder reconstruct the entire series and record the reconstruction error $e^{(u)}_t$.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/autoencoder.png&quot; width=&quot;70%&quot; /&gt;
    &lt;figcaption&gt;Processing an example sequence of (derived) sensor readings $\langle z_1, z_2, z_3 \rangle$ with the autoencoder. Illustration is taken from the original paper.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;\begin{equation}
e^{(u)}_t = || z^{(u)}_t - z’^{(u)}_t ||_2^2
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;health-index-hi&quot;&gt;Health Index (HI)&lt;/h4&gt;
&lt;p&gt;Based on the reconstruction error $e^{(u)}_t$ one can define a &lt;strong&gt;health index (HI)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;\begin{equation}
h^{(u)}_t = \frac{e^{(u)}_M - e^{(u)}_t}{e^{(u)}_M - e^{(u)}_m}
\end{equation}
with $e^{(u)}_M$ denoting the maximum error and $e^{(u)}_m$ denoting the minimum error.&lt;/p&gt;

&lt;p&gt;See that the reconstruction error is correlated with the machine’s health or lifetime respectively. The plots can be resproduced with the code provided in the repository.&lt;/p&gt;

&lt;figure class=&quot;once&quot; align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/rul/normalized_hi.pdf&quot; width=&quot;80%&quot; /&gt;
    &lt;figcaption&gt;Reconstruction error based health index averaged over all training instances (and normalized by their lifetime). The errorbars indicate one standard deviation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The actual RUL is estimated by function matching with all the HI curves in the training set. (This part is not in my implementation for now)&lt;/p&gt;

&lt;p&gt;Checkout my &lt;strong&gt;Tensorflow&lt;/strong&gt; implementation on &lt;a href=&quot;https://github.com/TimPhillip/rul_estimate&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Just run
&lt;code class=&quot;highlighter-rouge&quot;&gt;shell
python -m rul_estimate.turbofan_rul --train_ae
&lt;/code&gt;
to train the autoencoder part.&lt;/p&gt;

&lt;p&gt;Afterwards estimate the Health Index for the training instances with
&lt;code class=&quot;highlighter-rouge&quot;&gt;shell
python -m rul_estimate.turbofan_rul --target_hi
&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;notice-info&quot;&gt;&lt;strong&gt;Notice:&lt;/strong&gt; All ideas presented in this post are taken from the &lt;a href=&quot;https://arxiv.org/abs/1608.06154&quot;&gt;paper by Malhotra et. al.&lt;/a&gt;. All implementations are reproductions based on my understanding of the paper.&lt;/p&gt;</content><summary></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;teaser&quot;=&gt;&quot;turbofan.gif&quot;, &quot;feature&quot;=&gt;&quot;turbofan.png&quot;, &quot;credit&quot;=&gt;&quot;Photo by Inspirationfeed on Unsplash&quot;}" /></entry></feed>
