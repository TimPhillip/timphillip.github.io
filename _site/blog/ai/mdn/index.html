<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Mixture Density Networks • Tim Schneider</title>
    <meta name="description" content="



">
    <meta name="keywords" content="">
    
    	<!-- Twitter Cards -->
	<meta name="twitter:title" content="Mixture Density Networks">
	<meta name="twitter:description" content="



">
	
	
	
	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:image" content="/images/mdn/mdn_feature.png">
	
	<!-- Open Graph -->
	<meta property="og:locale" content="en">
	<meta property="og:type" content="article">
	<meta property="og:title" content="Mixture Density Networks">
	<meta property="og:description" content="



">
	<meta property="og:url" content="/blog/ai/mdn/">
	<meta property="og:site_name" content="Tim Schneider">

    <link rel="canonical" href="/blog/ai/mdn/">

    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Tim Schneider Atom Feed">
    <link href="/sitemap.xml" type="application/xml" rel="sitemap" title="Sitemap">

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="cleartype" content="on">

    <link rel="stylesheet" href="/css/main.css">
    <!-- HTML5 Shiv and Media Query Support for IE -->
    <!--[if lt IE 9]>
      <script src="/js/vendor/html5shiv.min.js"></script>
      <script src="/js/vendor/respond.min.js"></script>
    <![endif]-->

  </head>

  <body id="js-body">
    <!--[if lt IE 9]><div class="upgrade notice-warning"><strong>Your browser is quite old!</strong> Why not <a href="http://whatbrowser.org/">upgrade to a newer one</a> to better enjoy this site?</div><![endif]-->

    
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<header id="masthead">
  <div class="inner-wrap">
    <a href="/" class="site-title">Tim Schneider</a>
    <nav role="navigation" class="menu top-menu">
        <ul class="menu-item">
	<li class="home"><a href="/">Tim Schneider</a></li>
	
    
    <li><a href="/blog/" >Blog</a></li>
  
    
    <li><a href="/about/" >About</a></li>
  
    
    <li><a href="/contact/" >Contact</a></li>
  
</ul>
    </nav>
  </div><!-- /.inner-wrap -->
</header><!-- /.masthead -->
    <nav role="navigation" id="js-menu" class="sliding-menu-content">
  <h5>Tim Schneider <span>Table of Contents</span></h5>
  <ul class="menu-item">
    <li>
      <a href="/blog/">
        
        <div class="title">Blog</div>
        
      </a>
    </li><li>
      <a href="/about/">
        
        <div class="title">About</div>
        
      </a>
    </li><li>
      <a href="/contact/">
        
        <div class="title">Contact</div>
        
      </a>
    </li>
  </ul>
</nav>
<button type="button" id="js-menu-trigger" class="sliding-menu-button lines-button x2" role="button" aria-label="Toggle Navigation">
  <span class="nav-lines"></span>
</button>

<div id="js-menu-screen" class="menu-screen"></div>


    <div id="page-wrapper">
      <div id="main" role="main">
	<article class="wrap" itemscope itemtype="http://schema.org/Article">
		
		<div class="page-feature">
			<div class="page-image">
				<img src="/images/mdn/mdn_feature.png" class="page-feature-image" alt="Mixture Density Networks" itemprop="image">
				
			</div><!-- /.page-image -->
		</div><!-- /.page-feature -->
		
		
  <nav class="breadcrumbs">
    <span itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
      <a href="" itemprop="url">
        <span itemprop="title">Home</span>
      </a> › 
    <span itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
      <a href="/blog/" itemprop="url">
        <span itemprop="title">Blog</span>
      </a>
    </span>
  </nav><!-- /.breadcrumbs -->

		<div class="page-title">
			<h1>Mixture Density Networks</h1>
		</div>
		<div class="inner-wrap">
			<div id="content" class="page-content" itemprop="articleBody">
				<nav class="toc">

</nav>

<p>Some kind of patterns can not be captured with an ordinary feedfoward neuronal network. Take for example the function plotted below. Well it is not a function in mathematical sense, because the mapping $X \mapsto Y$ is ambiguous. So we can not use a function $X \mapsto Y$ to regress such a pattern.</p>

<p><img src="/images/mdn/mdn_prediction.png" alt="MDN prediction" /></p>

<p>Mixture Density Networks (MDN) model this pattern with a mixture of Gaussian distributions. The loss fucntion for this network can be derived from minimizing the negative log-likelihood of the data, w.r.t. the mixture of Gaussian distributions.</p>

<p>\begin{equation}
\mathcal{L}(y,x) = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{equation}</p>

<p>We can benefit from implementations already provided in <code class="highlighter-rouge">tf.contrib.distributions</code> for both, the <strong>categorical distribution</strong> $\Pi_k(x)$</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">OneHotCategorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
</code></pre>
</div>

<p>and the Gaussian parts $\mathcal{N}$ of the mixture</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
</code></pre>
</div>

<hr />

<p>We use a neuronal network to regress the parameters of the distributions:
We want $\pi$ to be a categorical distribution summing to one. This can be achieved by soft-max loss. The soft max loss for the logits $z$ of the mixture distribution $\pi$ is:
\begin{equation}
\pi_j = \sigma(z_j) = \frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}}
\end{equation}</p>

<p>For numerical stability reasons we do not regress $\pi$ directly with the network, but the probabilities in log-space $\log(\pi)$. This simplifies the soft-max as follows:</p>

<p>\begin{align}
\forall_j: \; \log(\pi_j) = \log\big(\sigma(z_j)\big) &amp; = \log\big(\frac{e^{z_j}}{\sum_{k=0}^{K} e^{z_k}} \big)
\end{align}</p>

<p>\begin{align}
= \log\big(e^{z_j} \big) - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}</p>

<p>\begin{align}
= z_j - \log \big( \sum_{k=0}^{K} e^{z_k} \big)
\end{align}</p>

<p>The second part of the formula can be computed numerically stable with</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>	<span class="n">tf</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre>
</div>

<p>Let’s see how the loss changes with these adaptions:</p>

<p>\begin{align}
\mathcal{L}(y,x) &amp; = - \log \Big[ \sum_{k=0}^{K} \Pi_k(x) \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}</p>

<p>\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big)} \cdot \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big]
\end{align}</p>

<p>\begin{align}
= - \log \Big[ \sum_{k=0}^{K} e^{\log\big(\pi_k(x) \big) + \log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) } \Big]
\end{align}</p>

<p>The probability of the Gaussian in log-space simplifies to:
\begin{align}
\log \Big( \mathcal{N}\big(y | \mu_k(x), \sigma_k(x) \big) \Big) &amp;= \log(\frac{1}{\sigma \sqrt{2\pi}}) + \log\Big(e^{- 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2} \Big)
\end{align}</p>

<p>\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \big( \frac{y - \mu_k}{\sigma} \big)^2
\end{align}</p>

<p>\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{\sigma ^2}
\end{align}</p>

<p>\begin{align}
= \log(\frac{1}{\sigma \sqrt{2\pi}}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}</p>

<p>\begin{align}
= -log(\sigma) - \log(\sqrt{2\pi}) - 0.5 \frac{ (y - \mu_k ) ^2 }{e^{2 * \log(\sigma)}}
\end{align}</p>

<p>The loss is now a function of $\log(\sigma), \log(\Pi)$ and $ \mu$ which we regress with the neural network.</p>

<hr />
<p>Additionally we can add a temperature parameter $\tau$ to control the uncertainty of our learned model as you can see below.
<img src="/images/mdn/mdn_temperatures.gif" alt="MDN temperature" /></p>

<hr />

<p>If we cut a slice of the plot for $X=0$ we can see the learned mixture of Gaussians for the distribution $\mathcal{P}(Y)$ over outcomes $Y$ :
<img src="/images/mdn/mixture_pdf.png" alt="pdf" /></p>


				<hr />
				<footer class="page-footer">
					

<div class="author-image">
	<img src="/images/bio-tim.jpg" alt="Tim Schneider">
</div><!-- ./author-image -->
<div class="author-content">
	<h3 class="author-name" >Written by <span itemprop="author">Tim Schneider</span></h3>
	<p class="author-bio"></p>
</div><!-- ./author-content -->
					<div class="inline-btn">
	<a class="btn-social twitter" href="https://twitter.com/intent/tweet?text=Mixture%20Density%20Networks&amp;url=/blog/ai/mdn/&amp;via=" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i> Share on Twitter</a>
	<a class="btn-social facebook" href="https://www.facebook.com/sharer/sharer.php?u=/blog/ai/mdn/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i> Share on Facebook</a>
	<a class="btn-social google-plus"  href="https://plus.google.com/share?url=/blog/ai/mdn/" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i> Share on Google+</a>
</div><!-- /.share-this -->

					<div class="page-meta">
	<p>Updated <time datetime="2018-09-02T20:00:00Z" itemprop="datePublished">September 02, 2018</time></p>
</div><!-- /.page-meta -->
				</footer><!-- /.footer -->
				<aside>
					
				</aside>
			</div><!-- /.content -->
		</div><!-- /.inner-wrap -->
		
	</article><!-- ./wrap -->
</div><!-- /#main -->

      <footer role="contentinfo" id="site-footer">
	<nav role="navigation" class="menu bottom-menu">
		<ul class="menu-item">
		
      
			<li><a href="" ></a></li>
		
		</ul>
	</nav><!-- /.bottom-menu -->
	<p class="copyright">&#169; 2018 <a href="">Tim Schneider</a> powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> + <a href="http://mmistakes.github.io/skinny-bones-jekyll/" rel="nofollow">Skinny Bones</a>.</p>
</footer>
    </div>

    <script src="/js/vendor/jquery-1.9.1.min.js"></script>
    <script src="/js/main.js"></script>

  </body>

</html>
